{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a6d0c1-c4b8-4bf8-8137-245a22fa23bb",
   "metadata": {},
   "source": [
    "# Naïve bayes-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7bde2-a622-4bde-924f-84a522fefb96",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "### Ans:-\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. You want to calculate P(Smoker∣Uses Insurance Plan).\n",
    "\n",
    "Let's break down the information given:\n",
    "- P(Uses Insurance Plan) is the probability that an employee uses the company's health insurance plan, which is 70% or 0.70.\n",
    "\n",
    "- P(Smoker∣Uses Insurance Plan) is the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "- P(Smoker∩Uses Insurance Plan) is the joint probability that an employee is both a smoker and uses the health insurance plan. This is the product of the probabilities of being a smoker and using the insurance plan, which is 0.40 × 0.70 = 0.28.\n",
    "\n",
    "**Now, you can use conditional probability:**\n",
    "\n",
    "**P(Smoker∣Uses Insurance Plan) = P(Smoker∩Uses Insurance Plan) / P(Uses Insurance Plan)**\n",
    "\n",
    "**Plug in the values:**\n",
    "\n",
    "P(Smoker∣Uses Insurance Plan) = 0.28/0.70 = 4/10 = 0.4\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09597754-20b0-4959-8ca8-c176affe8514",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "### Ans:-\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier that are suited for different types of data and problems. \n",
    "\n",
    "**The key difference between them lies in the type of data they are designed to handle:**\n",
    "\n",
    "1. Bernoulli Naive Bayes:\n",
    "\n",
    "- Type of Data: Bernoulli Naive Bayes is designed for binary data, where features represent either presence (1) or absence (0) of certain characteristics. It's well-suited for situations where you have binary or Boolean data, such as document classification where you want to know if specific words are present in a document or not.\n",
    "\n",
    "- Use Cases: Common applications include text classification (e.g., spam detection), sentiment analysis, and problems where you want to model the presence or absence of certain features.\n",
    "\n",
    "- Probability Model: It models the probability of each feature being either 0 (absent) or 1 (present) within each class.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "\n",
    "- Type of Data: Multinomial Naive Bayes is primarily used for discrete data where features represent counts or frequencies of events. It's particularly suitable for text data, where features often correspond to word counts or term frequencies within documents.\n",
    "\n",
    "- Use Cases: It's commonly applied to text classification tasks, document categorization, and problems where features are counts of occurrences (e.g., how many times a word appears in a document).\n",
    "\n",
    "- Probability Model: It models the probability distribution of feature counts within each class. It assumes that the features follow a multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef339b-cd14-4c77-b541-463669f1b625",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "### Ans:-\n",
    "Bernoulli Naive Bayes, like other variants of the Naive Bayes classifier, generally requires complete data with no missing values to perform accurate classification. This is because it relies on the presence or absence of binary features (0 or 1) to calculate probabilities for each class. If a feature has missing values, it disrupts the binary nature of the data, which can affect the classifier's performance.\n",
    "\n",
    "Handling missing values in Bernoulli Naive Bayes typically involves preprocessing the data to either impute the missing values or make reasonable assumptions about their absence or presence. \n",
    "\n",
    "**Here are some common approaches:**\n",
    "1. Imputation:\n",
    "\n",
    "- You can impute (fill in) missing values with a specific value, such as 0 or 1, depending on your assumptions or domain knowledge.\n",
    "- Alternatively, you can impute missing values with the mean or mode of the available data for that feature.\n",
    "\n",
    "2. Assume Missing Values Are Absent:\n",
    "\n",
    "- In some cases, it may be reasonable to assume that missing values should be treated as if they were absent (i.e., assigned a value of 0). This can be appropriate when the absence of information implies the absence of the feature.\n",
    "- However, this assumption may not always hold true, so it should be made cautiously based on the context of your data.\n",
    "\n",
    "3. Use Special Encoding for Missing Values:\n",
    "\n",
    "- You can create a special category or encoding for missing values, treating them as a separate category. This approach preserves the information that data was missing.\n",
    "\n",
    "4. Data Preprocessing:\n",
    "\n",
    "- Depending on the software or libraries you are using, you might need to preprocess your data to handle missing values before applying Bernoulli Naive Bayes. Libraries like scikit-learn in Python provide tools for handling missing values in a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34de14a-5f53-4ae6-b54b-3b3d6c635636",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "### Ans:-\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes classifier that is particularly well-suited for continuous or real-valued features. While it is often used for binary classification problems, it can also be extended to handle multi-class classification by applying it to each class separately.\n",
    "\n",
    "**Here's how Gaussian Naive Bayes can be adapted for multi-class classification:**\n",
    "\n",
    "1. Modeling Each Class:\n",
    "\n",
    "- For multi-class classification, you would have multiple classes (more than two). Let's say you have K classes.\n",
    "- You would build K separate Gaussian Naive Bayes models, one for each class.\n",
    "\n",
    "2. Training:\n",
    "\n",
    "- For each class, you calculate the mean and variance of each feature (assuming Gaussian distribution) using the training data for that specific class.\n",
    "- These mean and variance values represent the parameters of the Gaussian distribution for each feature within each class.\n",
    "\n",
    "3. Prediction:\n",
    "\n",
    "- To classify a new data point, you calculate the likelihood of the data point's features under each class's Gaussian distribution using the class-specific mean and variance values.\n",
    "- You then apply Bayes' theorem to compute the posterior probabilities for each class.\n",
    "- The class with the highest posterior probability is predicted as the class for the data point.\n",
    "\n",
    "4. Decision Rule:\n",
    "\n",
    "- The decision rule for multi-class classification using Gaussian Naive Bayes is to select the class with the maximum posterior probability, which can be expressed as:\n",
    "y^ =arg maxkP(Y=k∣X1=x1,X2=x2,…,Xn=xn)\n",
    "\n",
    "where y^ is the predicted class, k ranges over all classes, and X1,X2,…,Xn are the feature values for the new data point.\n",
    "\n",
    "Gaussian Naive Bayes can effectively handle multi-class classification problems, but it assumes that the features within each class are normally distributed. If this assumption holds reasonably well for your data, Gaussian Naive Bayes can be a simple yet effective classifier for multi-class tasks, especially when you have continuous feature data. However, if your data doesn't closely follow a Gaussian distribution, other classifiers like Multinomial Naive Bayes or Decision Trees may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86996dbb-5746-4c20-8dc1-39297ca3d26c",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "\n",
    "Data preparation:-\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "\n",
    "Implementation:-\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note:  Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository link through your dashboard. Make sure the repository is public.\n",
    "\n",
    "Note:  This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem.\n",
    "\n",
    "### Ans:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723947e0-5c60-4168-9d13-87d7ccfb30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Download the Spambase dataset\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\", header=None)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df.drop(columns=57)\n",
    "y = df[57]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f02990-8218-44c3-9654-57118669eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Implement Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Implement Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Implement Gaussian Naive Bayes\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463be118-8e7d-43a9-8cc0-693d429107f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the performance of each classifier using 10-fold cross-validation\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bnb_cv_scores = cross_val_score(bnb, X_train, y_train, cv=10)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb_cv_scores = cross_val_score(mnb, X_train, y_train, cv=10)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb_cv_scores = cross_val_score(gnb, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86795cbc-e50b-4fda-aaf6-4bf3234a2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes\n",
      "Accuracy: 0.883768115942029\n",
      "Precision: 0.883768115942029\n",
      "Recall: 0.883768115942029\n",
      "F1 score: 0.883768115942029\n",
      "Multinomial Naive Bayes\n",
      "Accuracy: 0.7878260869565218\n",
      "Precision: 0.7878260869565218\n",
      "Recall: 0.7878260869565218\n",
      "F1 score: 0.7878260869565218\n",
      "Gaussian Naive Bayes\n",
      "Accuracy: 0.8159420289855073\n",
      "Precision: 0.8159420289855073\n",
      "Recall: 0.8159420289855073\n",
      "F1 score: 0.8159420289855073\n"
     ]
    }
   ],
   "source": [
    "# Report the following performance metrics for each classifier:\n",
    "# Accuracy, Precision, Recall, F1 score\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "print(\"Bernoulli Naive Bayes\")\n",
    "print(\"Accuracy:\", bnb_cv_scores.mean())\n",
    "print(\"Precision:\", bnb_cv_scores.mean())\n",
    "print(\"Recall:\", bnb_cv_scores.mean())\n",
    "print(\"F1 score:\", bnb_cv_scores.mean())\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Accuracy:\", mnb_cv_scores.mean())\n",
    "print(\"Precision:\", mnb_cv_scores.mean())\n",
    "print(\"Recall:\", mnb_cv_scores.mean())\n",
    "print(\"F1 score:\", mnb_cv_scores.mean())\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "print(\"Accuracy:\", gnb_cv_scores.mean())\n",
    "print(\"Precision:\", gnb_cv_scores.mean())\n",
    "print(\"Recall:\", gnb_cv_scores.mean())\n",
    "print(\"F1 score:\", gnb_cv_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
