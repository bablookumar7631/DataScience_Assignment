{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e89679-4b73-4267-b37a-6c7ba3c33444",
   "metadata": {},
   "source": [
    "# Support Vector Machines-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f628e-4d53-414e-b6e3-5adf7b9ce8c5",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "### Ans:-\n",
    "\n",
    "Polynomial functions and kernel functions are related concepts in machine learning algorithms, particularly in the context of Support Vector Machines (SVMs) and kernel methods. Both are used to transform data into higher-dimensional spaces to handle non-linearity, but they serve slightly different purposes.\n",
    "\n",
    "**Here's the relationship between polynomial functions and kernel functions:**\n",
    "\n",
    "**1. Polynomial Functions:**\n",
    "\n",
    "- Polynomial functions are mathematical functions that take an input and transform it into a higher-dimensional space using polynomial terms. For example, a polynomial transformation of a one-dimensional input x might produce a higher-dimensional feature space with terms like x^2,x^3,etc.\n",
    "\n",
    "- In machine learning, polynomial functions are often used as explicit feature transformations. When you apply a polynomial transformation to your data, you create new features based on polynomial combinations of the original features. This can be useful when dealing with non-linear relationships in the data.\n",
    "\n",
    "For example, if you have a two-dimensional feature space with features x and y, applying a second-degree polynomial transformation would create features like x^2,y^2 and xy. This can be helpful when a simple linear model cannot capture the underlying patterns.\n",
    "\n",
    "**2. Kernel Functions:**\n",
    "\n",
    "- Kernel functions, on the other hand, are a broader concept used primarily in kernel methods like SVMs. A kernel function computes the similarity (usually a dot product) between data points in a higher-dimensional space without explicitly transforming the data. The key advantage is that it avoids the computational cost of explicitly transforming the data, especially in high-dimensional spaces.\n",
    "\n",
    "- Polynomial kernel functions are a specific type of kernel function. The polynomial kernel computes the dot product between data points after they have been implicitly transformed into a higher-dimensional space using polynomial terms. The formula for a polynomial kernel is often K(x,x')=(x.x'+c)^d, where c and d are parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce5353-377a-4f4e-8a74-eb0ddd317b5f",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "### Ans:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079e518-fb12-492d-be8e-f26ce87ccacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425cb396-88ee-4010-9d49-ddd1c7ad503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bb45e-6a4f-4d6f-ae55-ac0a566e7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176dde48-d94e-4313-9bc7-b99be39f3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d53e40-e57f-406b-b469-7fc3eb562c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469d3a1-9bd7-457f-ac69-5bb6402be31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bcb25-6379-4fe9-add8-83f09eb2e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49644a8b-0ef5-4129-abe8-35fb3910203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1167fe-5539-4c37-b991-471e2bafd258",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "### Ans:-\n",
    "\n",
    "Increasing the value of epsilon in SVR will decrease the number of support vectors.\n",
    "\n",
    "In SVR, the value of epsilon determines the width of the tube around the estimated function (hyperplane). The support vectors are the points that fall outside this tube. So, if the value of epsilon is increased, the tube will become wider, and fewer points will fall outside it. This will result in fewer support vectors.\n",
    "\n",
    "Here is an intuitive explanation of how this works. Imagine that we have a set of data points that are scattered randomly. We want to fit a regression line to these data points. If we use a small value of epsilon, the regression line will be very close to all of the data points. This will result in a large number of support vectors, since many of the data points will fall outside the tube around the regression line.\n",
    "\n",
    "On the other hand, if we use a large value of epsilon, the regression line will be further away from the data points. This will result in a smaller number of support vectors, since fewer of the data points will fall outside the tube around the regression line.\n",
    "\n",
    "In general, a small value of epsilon will result in a more accurate fit to the data, but it will also result in more support vectors. A large value of epsilon will result in a less accurate fit to the data, but it will also result in fewer support vectors.\n",
    "\n",
    "The best value of epsilon to use will depend on the specific dataset and the desired trade-off between accuracy and the number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1f548-f32a-496b-b6af-65b94c2b844d",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "### Ans:-\n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter can affect the performance of Support Vector Regression (SVR) in different ways.\n",
    "\n",
    "- Kernel function: The kernel function determines how the data is transformed into a higher-dimensional space. The most common kernel functions for SVR are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel. The linear kernel is the simplest kernel function and is used when the data is linearly separable. The polynomial kernel can be used when the data is not linearly separable, but it is more complex than the linear kernel. The RBF kernel is the most versatile kernel function and can be used for a wide variety of data.\n",
    "\n",
    "- C parameter: The C parameter controls the trade-off between the training error and the complexity of the model. A small value of C will result in a simpler model with a lower training error, but it may be more likely to overfit the data. A large value of C will result in a more complex model with a higher training error, but it may be less likely to overfit the data.\n",
    "\n",
    "- Epsilon parameter: The epsilon parameter controls the width of the tube around the regression line. A small value of epsilon will result in a more accurate fit to the data, but it may also result in more support vectors. A large value of epsilon will result in a less accurate fit to the data, but it may also result in fewer support vectors.\n",
    "\n",
    "- Gamma parameter: The gamma parameter controls the influence of each training example. A small value of gamma will result in each training example having a small influence on the model. A large value of gamma will result in each training example having a large influence on the model.\n",
    "\n",
    "**examples of when you might want to increase or decrease the value of each parameter:**\n",
    "\n",
    "- Kernel function: If the data is linearly separable, you can use the linear kernel. If the data is not linearly separable, you can use the polynomial kernel or the RBF kernel.\n",
    "- C parameter: If you are concerned about overfitting, you can use a small value of C. If you are not concerned about overfitting, you can use a large value of C.\n",
    "- Epsilon parameter: If you want to get a more accurate fit to the data, you can use a small value of epsilon. If you want to get a less accurate fit to the data, you can use a large value of epsilon.\n",
    "- Gamma parameter: If you want each training example to have a small influence on the model, you can use a small value of gamma. If you want each training example to have a large influence on the model, you can use a large value of gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d5dd9-6120-416f-89a2-3df7c7d61006",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataseg\n",
    "- Split the dataset into training and testing setZ\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "- Create an instance of the SVC classifier and train it on the training datW\n",
    "- hse the trained classifier to predict the labels of the testing datW\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-scoreK\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "- Train the tuned classifier on the entire dataseg\n",
    "- Save the trained classifier to a file for future use.\n",
    "\n",
    "Note: - You can use any dataset of your choice for this assignment, but make sure it is suitable for\n",
    "classification and has a sufficient number of features and samples.\n",
    "\n",
    "### Ans:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1235af-ce8d-4c57-b518-435acaebfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a25c4c-af8b-480d-9c27-ba7311325e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8996f6-8011-4cb0-a83f-e47e900799d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39807b92-6ea3-4c69-afe8-e2c299f697ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32389b77-a3d0-4d4c-ae0d-841e87fc25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SVC classifier\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28ab10-72b2-4749-b29f-3afeded54e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier on the training data\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21592edf-3612-476b-a7f8-c0bf589692f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the testing data\n",
    "y_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2939de8-d884-42ea-891a-9a4dfceacc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde459e1-3045-47c0-a1c0-e0fe42140a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "parameters = {'C': [1, 10, 100], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(svm, parameters, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628eb31-7e3a-483a-b3e8-8d4758d90f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384965a8-3b81-49ca-82c8-9dd6dd305c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tuned classifier on the entire dataset\n",
    "svm = grid_search.best_estimator_\n",
    "svm.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cda0b7-b920-414d-9cae-47003ce492ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier to a file\n",
    "with open('svm_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(svm, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
