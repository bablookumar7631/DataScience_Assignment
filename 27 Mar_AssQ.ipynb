{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7474bebe-7317-48a9-a352-9713d9e129eb",
   "metadata": {},
   "source": [
    "## Regression-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829d4dd-d3a2-43d6-973f-4abadf466ece",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "### Ans:-\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure used to assess the goodness-of-fit of a linear regression model. It provides information about how well the independent variables explain the variability in the dependent variable. R-squared is a value between 0 and 1, where higher values indicate a better fit of the model to the data.\n",
    "\n",
    "Mathematically, R-squared is calculated as follows:\n",
    "**R^2 = 1 - SSresidual / SStotal**\n",
    "\n",
    "Where:\n",
    "- SSresidual is the sum of squares of residuals (the differences between observed and predicted values).\n",
    "- SStotal is the total sum of squares, which measures the variability of the dependent variable around its mean.\n",
    "\n",
    ">Interpretation of R-squared:\n",
    "- R^2=0: The model explains none of the variability in the dependent variable, indicating that the model does not fit the data at all.\n",
    "- 0<R^2<1: The model explains a portion of the variability in the dependent variable. Higher R-squared values indicate that a larger proportion of the variability is explained by the model.\n",
    "- R^2=1: The model perfectly fits the data, explaining all the variability in the dependent variable. However, achieving an R-squared of 1 in practice is rare and can suggest overfitting.\n",
    "\n",
    ">Limitations of R-squared:\n",
    "\n",
    "- R-squared can be misleading if interpreted solely without considering other aspects of the model.\n",
    "- A high R-squared does not necessarily mean that the model is a good predictor. It's possible to overfit the data and achieve a high R-squared on the training data while the model may perform poorly on new, unseen data.\n",
    "- R-squared increases with the number of predictors, even if the added predictors have little or no explanatory power. Adjusted R-squared is often used to mitigate this issue.\n",
    "\n",
    ">Comparing Models: \n",
    "\n",
    "R-squared can be used to compare different models. A higher R-squared generally indicates a better fit, but it's important to consider the complexity of the model and whether the improvements in R-squared are practically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8581f-b9ac-4c1d-8e87-6048802c9153",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. \n",
    "\n",
    "### Ans:-\n",
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables (predictors) in a regression model. It addresses a limitation of the regular R-squared, which tends to increase as more predictors are added to the model, even if those predictors do not significantly contribute to explaining the variability in the dependent variable.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "**Adjusted R^2 = 1 - {(1-R^2).(n-1)} / (n-k-1)**\n",
    "\n",
    "Where:\n",
    "\n",
    "- R^2 is the regular R-squared.\n",
    "- n is the number of observations (data points).\n",
    "- k is the number of independent variables (predictors) in the model.\n",
    "\n",
    "\n",
    ">Differences between Adjusted R-squared and Regular R-squared:\n",
    "\n",
    "1. Penalty for Adding Predictors:-\n",
    "- Regular R-squared: It always increases or remains the same when adding more predictors, regardless of whether those predictors have any real explanatory power.\n",
    "- Adjusted R-squared: It penalizes the addition of predictors that do not improve the model's fit enough to justify their inclusion. As the number of predictors increases, the adjusted R-squared will increase only if the added predictors genuinely contribute to explaining the variability in the dependent variable.\n",
    "\n",
    "2. Complexity Consideration:-\n",
    "- Regular R-squared: Does not consider the complexity of the model or the potential for overfitting.\n",
    "- Adjusted R-squared: Encourages a trade-off between model complexity and fit by considering both the fit (as measured by R-squared) and the number of predictors. This makes it more suitable for model selection.\n",
    "\n",
    "3. Comparison of Models:\n",
    "- Regular R-squared: Can lead to favoring models with more predictors, even if their contribution is marginal.\n",
    "- Adjusted R-squared: Provides a more balanced measure for comparing models with different numbers of predictors. It helps prevent the selection of overly complex models that might not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109dc86-933e-4a2a-8787-09b7b91badee",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "### Ans:-\n",
    "Adjusted R-squared is more appropriate to use when you are comparing and evaluating regression models that have different numbers of predictors. It provides a more balanced and meaningful measure of model goodness-of-fit, taking into account both the model's explanatory power and the number of predictors included.\n",
    "\n",
    ">Situations when adjusted R-squared is particularly useful:\n",
    "1. Model Comparison:- When you are considering multiple regression models with varying numbers of predictors, adjusted R-squared helps you compare these models more effectively. It considers not only the fit of the model to the data (as measured by the regular R-squared) but also the complexity introduced by the number of predictors.\n",
    "\n",
    "2. Avoiding Overfitting:- Adjusted R-squared helps in avoiding overfitting. Overfitting occurs when a model captures noise in the training data, leading to poor generalization to new data. By penalizing the inclusion of additional predictors that don't significantly contribute to model fit, adjusted R-squared encourages the selection of simpler models that are less likely to overfit.\n",
    "\n",
    "3. Model Selection:- When deciding which predictors to include in your model, adjusted R-squared can guide you toward a balance between model complexity and predictive performance. It helps you avoid adding too many predictors that might not improve the model's predictive ability enough to justify their inclusion.\n",
    "\n",
    "4. Interpretability:- Models with fewer predictors tend to be more interpretable. Adjusted R-squared encourages parsimonious models by considering both fit and complexity, which can lead to more straightforward interpretation.\n",
    "\n",
    "5. Generalization:- Models with higher adjusted R-squared values are more likely to generalize well to new, unseen data. The adjusted R-squared focuses on the trade-off between explaining the variability in the training data and preventing the introduction of unnecessary complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ada3b3-fd0f-4f23-9981-799bfc3a63b1",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "### Ans:-\n",
    "1. RMSE (Root Mean Squared Error):-\n",
    "RMSE is a measure of the average magnitude of the errors between the predicted and actual values. It gives more weight to larger errors, making it sensitive to outliers. The RMSE is calculated as follows:\n",
    "\n",
    "            n      ^\n",
    "**RMSE = √∑i=1(yi - yi) / n**\n",
    "         \n",
    "Where:\n",
    "- yi is the actual observed value for the ith data point.\n",
    "- yi^ is the predicted value for the ith data point.\n",
    "- n s the number of data points.\n",
    "\n",
    "2. MSE (Mean Squared Error):-\n",
    "MSE is the average of the squared errors between the predicted and actual values. Like RMSE, it gives more weight to larger errors and is sensitive to outliers. The MSE is calculated as:\n",
    "\n",
    "**MSE = 1/n ∑(yi - yi^)^2**\n",
    "\n",
    "3. MAE (Mean Absolute Error):\n",
    "MAE measures the average magnitude of the errors without considering their direction. It is less sensitive to outliers compared to RMSE and MSE. The MAE is calculated as:\n",
    "**MAE = 1/n ∑|yi - yi^|**\n",
    "\n",
    "where:\n",
    "- yi is the actual observed value for the ith data point.\n",
    "- yi^ is the predicted value for the ith data point.\n",
    "- n is the number of data points.\n",
    "\n",
    ">Interpretation:\n",
    "\n",
    "- All three metrics (RMSE, MSE, MAE) quantify the accuracy of predictions, with lower values indicating better performance.\n",
    "- RMSE and MSE give more weight to larger errors, making them more sensitive to outliers.\n",
    "- MAE is generally easier to interpret since it represents the average magnitude of the errors without squared terms.\n",
    "\n",
    ">Choosing the Metric:\n",
    "\n",
    "- The choice of metric depends on the problem and the goals of the analysis.\n",
    "- RMSE and MSE are suitable when larger errors are more critical and when the data contains outliers.\n",
    "- MAE is preferred when you want a more robust metric that's less influenced by outliers.\n",
    "- It's also common to use multiple metrics to get a more comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2a7c1-6f16-4b2f-bd9f-e282f1621115",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "### Ans:- \n",
    ">Advantages of Using RMSE, MSE, and MAE:\n",
    "1. Quantification of Error: These metrics provide a quantifiable measure of the difference between predicted and actual values, helping to assess the accuracy of regression models.\n",
    "\n",
    "2. Comparison: RMSE, MSE, and MAE allow for direct comparison of different models or variations of the same model, helping in model selection.\n",
    "\n",
    "3. Sensitivity to Large Errors: RMSE and MSE give more weight to larger errors, making them effective for capturing the impact of significant outliers.\n",
    "\n",
    "4. Ease of Interpretation: MAE has a straightforward interpretation as the average magnitude of errors, which can be easily understood by non-technical stakeholders.\n",
    "\n",
    ">Disadvantages and Considerations:\n",
    "\n",
    "1. Sensitivity to Outliers:\n",
    "- Advantage: RMSE and MSE are sensitive to outliers, which can be beneficial when these outliers are of importance.\n",
    "- Disadvantage: Sensitivity to outliers can also be a drawback in cases where outliers are noise or data anomalies rather than meaningful observations.\n",
    "\n",
    "2. Squared Errors:\n",
    "- Disadvantage: RMSE and MSE involve squaring errors, which can magnify the effect of larger errors and potentially lead to a skewed evaluation if outliers are present.\n",
    "\n",
    "3. Complexity vs. Robustness:\n",
    "- Advantage: RMSE and MSE provide a comprehensive evaluation by considering the magnitude of errors and their distribution.\n",
    "- Disadvantage: This complexity can also lead to an emphasis on fitting the training data too closely, potentially overfitting or losing generalization ability.\n",
    "\n",
    "4. Interpretability:\n",
    "- Advantage: MAE is easily interpretable as the average magnitude of errors, making it more accessible to non-technical stakeholders.\n",
    "- Disadvantage: RMSE and MSE, with their squared errors, can be less intuitive to interpret directly.\n",
    "\n",
    "5. Application Specific:\n",
    "- Advantage: Different metrics serve different purposes, allowing you to choose the most suitable metric based on the problem and objectives.\n",
    "- Disadvantage: This flexibility can also make it challenging to decide which metric to use, and using multiple metrics might be necessary for a comprehensive evaluation.\n",
    "\n",
    "6. Objective Consideration:\n",
    "- Consideration: The choice between RMSE, MSE, and MAE depends on the specific goals of the analysis, such as whether the emphasis is on outlier performance, balanced performance, or easy interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661f978-02df-45ef-be7e-74c85ea07444",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "### Ans:- \n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent overfitting by adding a penalty term to the model's cost function. The penalty term is based on the absolute values of the coefficients of the regression variables. Lasso encourages sparse solutions, meaning it tends to drive some of the coefficients to exactly zero, effectively performing feature selection and excluding less important predictors from the model.\n",
    "\n",
    ">Differences between Lasso and Ridge regularization:\n",
    "1. Penalty Term:-\n",
    "- Lasso: The penalty term added to the cost function is the absolute sum of the coefficients (|Wi|).\n",
    "- Ridge: The penalty term added to the cost function is the squared sum of the coefficients (Wj^2).\n",
    "\n",
    "2. Sparsity of Coefficients:-\n",
    "- Lasso: Lasso tends to drive some coefficients to exactly zero, resulting in a sparse model. This leads to feature selection, where some predictors are effectively excluded from the model.\n",
    "- Ridge: Ridge does not force coefficients to be exactly zero. It shrinks coefficients toward zero, but they still remain in the model.\n",
    "\n",
    "3. Feature Selection:-\n",
    "- Lasso: More suitable for feature selection when you suspect that only a subset of predictors are truly important.\n",
    "- Ridge: Does not perform feature selection well; it tends to keep all predictors in the model with small non-zero coefficients.\n",
    "\n",
    "4. Performance on Highly Correlated Predictors:\n",
    "- Lasso: Can arbitrarily select one of a group of correlated predictors and drive the rest to zero.\n",
    "- Ridge: Distributes the penalty more evenly among correlated predictors.\n",
    "\n",
    ">When to use Lasso regularization:-\n",
    "Lasso regularization is more appropriate when:-\n",
    "- You suspect that only a subset of predictors are relevant and want to perform feature selection.\n",
    "- You have a high-dimensional dataset with potentially many irrelevant predictors.\n",
    "- You are seeking a sparse model for interpretability.\n",
    "- You want to emphasize on driving some coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9edd6-88ca-47b7-9e1e-e9133af70bd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "### Ans:-\n",
    "Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by introducing a penalty term to the model's cost function. This penalty term discourages the model from fitting the training data too closely and from assigning overly large coefficients to the predictor variables. The goal is to find a balance between fitting the data well and keeping the model's complexity in check.\n",
    "\n",
    ">how regularized linear models achieve this and help prevent overfitting:\n",
    "1. Addition of Penalty Term:-\n",
    "In standard linear regression, the objective is to minimize the mean squared error (MSE) between the predicted and actual values. Regularized linear models modify this objective by adding a penalty term based on the coefficients of the predictor variables.\n",
    "\n",
    "2. Penalty on Coefficients:-\n",
    "The penalty term typically depends on the magnitude of the coefficients. For example, in Ridge regression, the penalty term is the sum of the squared coefficients (w1^2 + w2^2 + ... + wp^2).In Lasso regression, it's the sum of the absolute values of the coefficients(|w1| + |w2| + ... + |wp|).\n",
    "\n",
    "3. Trade-off Between Fit and Complexity:-\n",
    "By introducing the penalty term, regularized models seek to minimize both the MSE and the magnitude of the coefficients. This trade-off encourages the model to find a balance between fitting the training data and keeping the coefficients small.\n",
    "\n",
    "4. Controlled Complexity:\n",
    "The strength of the penalty is controlled by a hyperparameter (α). Larger values of α increase the penalty, which in turn leads to smaller coefficient values. This helps prevent the model from overfitting by reducing the complexity introduced by high coefficients.\n",
    "\n",
    "5. Feature Selection:-\n",
    "Regularized models like Lasso regression can even force some coefficients to become exactly zero. This effectively performs feature selection, excluding less important predictors from the model. This can be particularly helpful when dealing with high-dimensional data with many predictors.\n",
    "\n",
    "6. Better Generalization:-\n",
    "Regularized models produce simpler, more generalizable models that are less likely to memorize noise in the training data. This results in better performance on new, unseen data, which is the ultimate goal of machine learning.\n",
    "\n",
    ">Let's illustrate this with an example using Ridge regression:\n",
    "\n",
    "**Example: Predicting House Prices**\n",
    "Imagine you have a dataset containing information about houses, including features like square footage, number of bedrooms, and location. You want to build a linear regression model to predict house prices based on these features.\n",
    "\n",
    "**Overfitting Scenario:**\n",
    "In an overfitting scenario, you build a linear model that perfectly fits the training data by assigning very high coefficients to all features. This may lead to high accuracy on the training data but poor generalization to new data because the model has learned the noise and fluctuations in the training set.\n",
    "\n",
    "**Regularization to Prevent Overfitting:**\n",
    "Here's where Ridge regression comes in. Instead of just minimizing the squared errors between predicted and actual values, Ridge regression adds a penalty term to the cost function. The penalty term is the sum of squared coefficients multiplied by a regularization parameter (α). The higher the coefficients, the higher the penalty, encouraging the model to keep coefficients small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdf50d-0c2b-4f98-99bd-2bfe42d0d9d5",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "### Ans:- \n",
    "Regularized linear models, while effective in preventing overfitting and improving generalization, have their limitations and might not always be the best choice for regression analysis.\n",
    "\n",
    ">some limitations to consider:\n",
    "1. Loss of Interpretability:\n",
    "Regularized models, particularly Lasso regression, tend to shrink some coefficients to exactly zero, effectively excluding those predictors from the model. While this can be useful for feature selection, it also means that the model's interpretability might be compromised. It becomes challenging to explain the impact of excluded predictors on the outcome.\n",
    "\n",
    "2. Bias-Variance Trade-off:\n",
    "Regularized models trade off some bias (error due to underfitting) for reduced variance (error due to overfitting). While this trade-off can improve generalization, it might not be appropriate when low bias is crucial. In situations where a more complex model can capture important nuances in the data, regularized models might not capture those complexities well.\n",
    "\n",
    "3. Hyperparameter Tuning:\n",
    "Regularized models require tuning the hyperparameter (α in Ridge and Lasso) to strike the right balance between fit and complexity. Selecting the optimal value of the hyperparameter can be challenging and might require cross-validation, adding an extra layer of complexity to the modeling process.\n",
    "\n",
    "4. Assumption of Linearity:\n",
    "Like traditional linear regression, regularized linear models assume a linear relationship between predictors and the outcome. If the true relationship is significantly nonlinear, regularized models might not perform well and more flexible modeling approaches (like polynomial regression or non-linear models) might be more appropriate.\n",
    "\n",
    "5. Ineffectiveness with Few Predictors:\n",
    "Regularized models shine when dealing with high-dimensional data where overfitting is a concern. If you have a small number of predictors and a sufficient amount of data, simpler linear regression models without regularization might perform just as well and provide more straightforward interpretation.\n",
    "\n",
    "6. Impact of Outliers:\n",
    "Regularized models can be sensitive to outliers, particularly Lasso regression. The penalty terms might not differentiate well between outliers and meaningful observations, leading to unexpected coefficient behavior.\n",
    "\n",
    "7. Elastic Net for Balanced Solutions:\n",
    "While Ridge and Lasso each have their advantages, neither is universally superior. Elastic Net regularization combines both Ridge and Lasso penalties, aiming to provide a balanced approach. However, it introduces another hyperparameter (L1 ratio) to control the mix of penalties.\n",
    "\n",
    "Regularized linear models are powerful tools for preventing overfitting and improving generalization, they are not one-size-fits-all solutions. The choice between regularized and non-regularized models depends on the nature of the data, the goals of the analysis, and the trade-offs between model complexity, interpretability, and performance. Careful consideration of these factors is necessary to decide whether regularized linear models are the best fit for a specific regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a483f1-c612-43d4-8352-18c34e06fa5f",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics.Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "### Ans:-\n",
    "Choosing the better performing model depends on the specific goals and characteristics of the problem. Both RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are commonly used metrics to evaluate regression models, but they capture different aspects of the model's performance.\n",
    "\n",
    "- RMSE of 10 for Model A:- This metric emphasizes larger errors more than smaller ones due to the squared term in the calculation. It's sensitive to outliers and punishes the model more severely for predictions that deviate from the actual values.\n",
    "\n",
    "- MAE of 8 for Model B:- This metric considers the absolute magnitude of errors without squaring them. It treats all errors equally, regardless of their size, and is less sensitive to outliers.\n",
    "\n",
    ">Choosing the better model depends on the nature of the problem:\n",
    "\n",
    "1. If Small Errors Matter Equally Everywhere:-\n",
    "If you care about minimizing errors consistently across all observations and don't want larger errors to have a disproportionate impact, Model B with the lower MAE might be preferred. MAE provides a straightforward measure of the average absolute error, making it a good choice when all errors are equally important.\n",
    "\n",
    "2. If Larger Errors Matter More:-\n",
    "If larger errors are more concerning and you want to heavily penalize predictions that deviate significantly from the actual values, then Model A with the RMSE of 10 might be more appropriate. RMSE places more emphasis on larger errors and can be a better choice when you want to account for the magnitude of errors.\n",
    "\n",
    ">Limitations and Considerations:\n",
    "\n",
    "- Outliers: Both metrics are sensitive to outliers, but RMSE is more sensitive due to the squared term. If your dataset contains outliers that are significant to the problem, RMSE might be disproportionately influenced.\n",
    "\n",
    "- Scale of the Dependent Variable: RMSE is influenced by the scale of the dependent variable. If your dependent variable is on a larger scale, the RMSE values might also be larger compared to when the dependent variable is on a smaller scale. MAE is not as affected by scale.\n",
    "\n",
    "- Model Goals: The choice of metric should align with the goals of the analysis and the context of the problem. Consider whether small errors across the board or large errors for specific cases are more critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b396365-3d42-4613-a59b-0564b6557b60",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularizationmethod?\n",
    "\n",
    "### Ans:-\n",
    "Choosing the better performing regularized linear model between Model A (Ridge regularization) and Model B (Lasso regularization) depends on the specific goals of the analysis, the nature of the data, and the trade-offs associated with each type of regularization.\n",
    "\n",
    "**Model A: Ridge Regularization (Regularization Parameter = 0.1):**\n",
    "Ridge regression adds a penalty term based on the sum of squared coefficients to the cost function. The regularization parameter (α) controls the strength of the penalty. Smaller values of α allow the coefficients to be larger, whereas larger values of α encourage the coefficients to be smaller.\n",
    "\n",
    ">**Choosing the Better Model:**\n",
    "\n",
    "The choice between Ridge and Lasso regularization depends on the following considerations:\n",
    "\n",
    "1. Feature Selection:\n",
    "If you suspect that some predictors are less important or irrelevant to the outcome, and you want a sparse model with feature selection, Lasso regularization (Model B) might be preferred. Lasso tends to drive some coefficients to zero, effectively excluding those predictors from the model.\n",
    "\n",
    "2. Balancing Complexity:\n",
    "Ridge regularization (Model A) tends to distribute the penalty more evenly across all predictors without driving coefficients exactly to zero. This can be beneficial when you want to reduce the influence of less important predictors without completely excluding them.\n",
    "\n",
    "3. Interpretability:\n",
    "Lasso's feature selection property can improve model interpretability by simplifying the model and focusing on a subset of predictors. However, if all predictors are genuinely relevant, Ridge might be preferable to avoid excluding meaningful variables.\n",
    "\n",
    "4. Performance on Test Data:\n",
    "Ultimately, the choice should be guided by the model's performance on unseen test data. Cross-validation or hold-out validation should be used to assess which model generalizes better to new data.\n",
    "\n",
    ">**Trade-offs and Limitations:**\n",
    "\n",
    "1. Ridge:\n",
    "- Ridge regularization doesn't drive coefficients exactly to zero. If you want to completely exclude predictors, Ridge might not be the best choice.\n",
    "- Ridge doesn't perform variable selection as well as Lasso when you have a strong suspicion that many predictors are irrelevant.\n",
    "\n",
    "2. Lasso:\n",
    "- Lasso's feature selection might lead to a loss of important information if predictors that should be included are driven to zero.\n",
    "- Lasso can be sensitive to correlated predictors; it tends to select one of the correlated predictors and exclude the rest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
