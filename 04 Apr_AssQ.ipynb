{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31477204-7daf-4a03-be9d-62f577205070",
   "metadata": {},
   "source": [
    "## Decision Tree-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff7503-46b9-4999-9b10-8245f505fbeb",
   "metadata": {},
   "source": [
    "### Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "\n",
    "### Ans:-\n",
    "A Decision Tree Classifier is a supervised machine learning algorithm used for both classification and regression tasks. It's a versatile and interpretable algorithm that works by recursively partitioning the dataset into subsets based on the features to make predictions.\n",
    "\n",
    "**Here's a step-by-step description of how the Decision Tree Classifier algorithm works to make predictions:**\n",
    "\n",
    "1. Initialization: The algorithm starts with the entire dataset, which consists of a set of labeled examples. Each example has a set of features (attributes) and a corresponding class label (the target variable).\n",
    "\n",
    "2. Feature Selection: The algorithm evaluates different features to determine which one is the best to split the dataset. It does this by calculating a measure of impurity or information gain for each feature. Common measures of impurity include Gini impurity, entropy, or classification error.\n",
    "\n",
    "3. Splitting the Dataset: Once the best feature is selected, the dataset is split into subsets based on the values of that feature. Each subset corresponds to a different branch or node of the decision tree.\n",
    "\n",
    "4. Recursive Process: Steps 2 and 3 are repeated recursively for each subset created in the previous step. The algorithm continues to split the data until one of the stopping criteria is met. Stopping criteria could be a maximum tree depth, a minimum number of samples in a node, or when a node becomes pure (contains only one class).\n",
    "\n",
    "5. Assigning Class Labels: When a stopping criterion is met for a node, it's assigned a class label. In a classification problem, this label is determined by a majority vote of the examples in that node. For regression problems, it might be the mean or median of the target values in that node.\n",
    "\n",
    "6. Tree Pruning (optional): After the tree is fully grown, it may be pruned to prevent overfitting. Pruning involves removing branches or nodes that do not significantly contribute to the model's predictive power.\n",
    "\n",
    "7. Prediction: To make a prediction for a new, unseen example, you start at the root node of the tree and traverse down the tree following the feature splits according to the values of the example's features. Eventually, you reach a leaf node, which provides the predicted class label.\n",
    "\n",
    "8. Output: The predicted class label is the output of the Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302906f7-dd5c-4acd-9592-ee847c3bc380",
   "metadata": {},
   "source": [
    "### Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "\n",
    "### Ans:-\n",
    "The mathematical intuition behind Decision Tree Classification involves concepts of information theory and impurity measures.\n",
    "\n",
    "**I'll explain the key mathematical concepts step by step:**\n",
    "\n",
    "1. Entropy:\n",
    "- Entropy, denoted as H(S), is a measure of impurity or randomness in a dataset.\n",
    "- For a binary classification problem (two classes, often labeled 0 and 1), the entropy formula is given by:\n",
    "**H(S)=−p0 ∗ log2(p0)−p1∗log2(p1)**\n",
    "- Here,p0is the proportion of examples in class 0, and p1 is the proportion in class 1 in the dataset S.\n",
    "- The entropy is 0 when the dataset is pure (contains only one class), and it is higher when the dataset is more mixed.\n",
    "\n",
    "2. Information Gain:\n",
    "- Information Gain (IG) measures the reduction in entropy achieved by partitioning a dataset based on a specific feature.\n",
    "- The formula for Information Gain is as follows:\n",
    "**IG(S, A) = H(S) - ∑v∈Values(A) |S0|/|S| * H(Sv)**\n",
    "- IG(S,A) is the Information Gain by splitting dataset S using feature A.\n",
    "- H(S) is the entropy of the original dataset S.\n",
    "- Values(A) represents the possible values of feature A.\n",
    "- Sv is the subset of examples in S where feature A takes the value v.\n",
    "- The goal is to select the feature A that maximizes Information Gain, as it will provide the most significant reduction in uncertainty about the class labels.\n",
    "\n",
    "3. Gini Impurity:\n",
    "- Another common measure of impurity is Gini Impurity, denoted as Gini(S).\n",
    "- For a binary classification problem, the Gini Impurity formula is:\n",
    "**Gini(S) = 1 - (po^2 + p1^2)**\n",
    "- Similar to entropy, Gini Impurity is 0 when the dataset is pure and increases as the dataset becomes more mixed.\n",
    "\n",
    "4. CART Algorithm:\n",
    "- Decision Trees often use the CART (Classification and Regression Trees) algorithm.\n",
    "- For classification tasks, CART aims to minimize the Gini Impurity or maximize the reduction in Gini Impurity when choosing a split.\n",
    "- The Gini Impurity for a split on feature A is calculated as:\n",
    "**Gini(S, A) = ∑v∈Values(A) |S0|/|S| * Gini(Sv)**\n",
    "- The goal is to select the feature A and split value that minimizes the weighted average of the Gini Impurity for the resulting subsets.\n",
    "\n",
    "5. Recursive Splitting:\n",
    "- Decision Trees work by recursively selecting the feature that maximizes Information Gain (or minimizes Gini Impurity) at each node.\n",
    "- The process continues until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf, or pure leaves).\n",
    "- At each internal node, the tree selects the best split, and at each leaf node, it assigns a class label based on a majority vote of the training examples that reached that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156aead3-82c6-4716-8d54-f398bd869fe4",
   "metadata": {},
   "source": [
    "### Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "\n",
    "### Ans:-\n",
    "A Decision Tree Classifier can be used to solve a binary classification problem, where the goal is to categorize input data into one of two possible classes, often denoted as 0 and 1. \n",
    "\n",
    "let's walk through how a Decision Tree Classifier can be used to solve a binary classification problem using an example of classifying whether an email is spam or not based on two features: the number of exclamation marks and the presence of certain keywords.\n",
    "\n",
    "1. Data Preparation:\n",
    "- You start with a labeled dataset containing emails. Each email is represented by its features (number of exclamation marks, presence of keywords) and a class label (spam or not spam).\n",
    "\n",
    "2. Building the Tree:\n",
    "- The Decision Tree algorithm starts by selecting the feature that best separates the data based on its impurity measure (e.g., Information Gain or Gini Impurity).\n",
    "- Let's say the number of exclamation marks is the best feature to split on initially. The algorithm creates a decision node using this feature.\n",
    "\n",
    "3. Creating Branches:\n",
    "- The dataset is split into subsets based on the values of the chosen feature (exclamation marks). For example, emails with fewer than 3 exclamation marks might go to the left branch, and emails with 3 or more might go to the right branch.\n",
    "\n",
    "4. Continuing the Process:\n",
    "- For each branch, the algorithm repeats the process of selecting the best feature to split on. This continues until a stopping criterion is met (e.g., maximum tree depth, minimum samples per leaf, or pure leaves).\n",
    "\n",
    "5. Leaf Nodes:\n",
    "- When a stopping criterion is reached, a leaf node is created. Each leaf node is assigned the majority class label of the examples that reach that node.\n",
    "- For instance, if most emails with few exclamation marks are not spam, the corresponding leaf node will be labeled as \"not spam.\"\n",
    "\n",
    "6. Prediction:\n",
    "- To classify a new email, you start at the root node and traverse the tree by following the decisions based on the features of the email.\n",
    "- At each decision node, you compare the feature value of the email to the split value of the node and move down the appropriate branch.\n",
    "- Once you reach a leaf node, the class label associated with that node is your prediction.\n",
    "\n",
    "7. Example Prediction:\n",
    "- Let's say you want to classify an email with 2 exclamation marks and contains the specified keywords.\n",
    "- You start at the root node (based on exclamation marks) and follow the branch for emails with fewer than 3 exclamation marks.\n",
    "- You might reach a leaf node labeled \"not spam,\" which becomes your prediction for the given email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1084ff1-9290-496d-a913-c96414eec96a",
   "metadata": {},
   "source": [
    "### Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\n",
    "\n",
    "### Ans:-\n",
    "The geometric intuition behind Decision Tree Classification involves viewing the process as dividing the feature space into distinct regions or partitions, each associated with a specific class label. This approach can help in understanding how Decision Trees make predictions and why they are sometimes referred to as piecewise-constant classifiers. \n",
    "\n",
    "**Here's a step-by-step explanation of the geometric intuition:**\n",
    "\n",
    "1. Feature Space:\n",
    "- Imagine the feature space as a multi-dimensional space where each axis represents a feature (e.g., features A and B). The data points (examples) are scattered across this space based on their feature values.\n",
    "\n",
    "2. Partitioning:\n",
    "- The Decision Tree algorithm identifies feature thresholds or splits that divide the feature space into regions. These splits are aligned with the feature axes.\n",
    "- For example, the first split might be on feature A at a certain threshold value, creating two regions in the feature space: one where A is less than the threshold and another where A is greater than or equal to the threshold.\n",
    "\n",
    "3. Recursive Splitting:\n",
    "- The partitioning process is recursive. Each region created by a split is further divided based on another feature or threshold until a stopping criterion is met.\n",
    "- This recursive splitting continues until either a maximum tree depth is reached, there are too few examples in a region, or the region becomes pure (contains only one class).\n",
    "\n",
    "4. Regions Correspond to Class Labels:\n",
    "- Each terminal node (leaf) of the Decision Tree corresponds to a region in the feature space.\n",
    "- The class label assigned to a region (leaf node) is determined by the majority class of the training examples that fall within that region.\n",
    "\n",
    "5. Decision Boundaries:\n",
    "- The decision boundaries in the feature space are formed by the splits. At each split, the algorithm decides which side of the boundary a data point falls on based on the feature values.\n",
    "- Decision boundaries are typically orthogonal to the feature axes because Decision Trees make axis-aligned splits.\n",
    "\n",
    "6. Prediction:\n",
    "- To make a prediction for a new data point, you start at the root node of the tree and traverse down the tree based on the feature values of the data point.\n",
    "- At each internal node, you compare the feature value to the threshold and decide whether to move left or right along the decision boundary.\n",
    "- You continue this process until you reach a leaf node, which assigns a class label to the data point.\n",
    "\n",
    "7. Example Prediction:\n",
    "- Suppose you're classifying two-dimensional data points as either class A or class B.\n",
    "- The Decision Tree creates splits along the feature axes, effectively partitioning the space into regions.\n",
    "- When you have a new data point (e.g., [x, y]), you follow the splits to determine which region it falls into and assign the majority class label of that region as the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e861ee8-912b-4b46-91e7-d7cb9b54416f",
   "metadata": {},
   "source": [
    "### Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n",
    "\n",
    "### Ans:-\n",
    "A confusion matrix is a fundamental tool used to evaluate the performance of a classification model, particularly in binary and multi-class classification problems. It provides a clear and concise summary of the model's predictions compared to the actual class labels in a dataset. A confusion matrix is also known as an error matrix.\n",
    "\n",
    "**Usage of a Confusion Matrix:**\n",
    "The confusion matrix provides valuable information for evaluating the performance of a classification model. Here's how it can be used:\n",
    "\n",
    "1. Accuracy: Accuracy is a common metric and is calculated as:\n",
    "**Accuracy = (TP + TN) / (TP+TN+FP+FN)**\n",
    "\n",
    "It represents the proportion of correctly classified instances out of all instances in the dataset. While accuracy is informative, it may not be the best metric in imbalanced datasets where one class significantly outweighs the other.\n",
    "\n",
    "2. Precision (Positive Predictive Value): Precision measures the accuracy of positive predictions and is calculated as:\n",
    "**Precision = TP / TP+FP**\n",
    "\n",
    "It tells you how many of the positive predictions made by the model were actually correct. High precision indicates that when the model predicts a positive class, it's often correct.\n",
    "\n",
    "3. Recall (Sensitivity, True Positive Rate): Recall quantifies the model's ability to correctly identify positive instances and is calculated as:\n",
    "**Recall = TP / TP+FN**\n",
    "\n",
    "It tells you how many of the actual positive instances the model correctly classified. High recall means the model is good at finding positive instances.\n",
    "\n",
    "4. F1-Score: The F1-score is the harmonic mean of precision and recall and is used when you want to balance the trade-off between precision and recall:\n",
    "**F1 = 2.Precision.Recall / Precision + Recall**\n",
    "\n",
    "5. Specificity (True Negative Rate): Specificity measures the model's ability to correctly identify negative instances and is calculated as:\n",
    "**Specificity = TN / TN+FN**\n",
    "\n",
    "It is particularly useful when the negative class is of particular importance.\n",
    "\n",
    "6. Receiver Operating Characteristic (ROC) Curve: The ROC curve is a graphical representation of the trade-off between true positive rate (recall) and false positive rate (1-specificity) at various classification thresholds. It helps you visualize the model's performance across different threshold values.\n",
    "\n",
    "7. Area Under the ROC Curve (AUC-ROC): AUC-ROC quantifies the overall performance of a model. A higher AUC-ROC indicates a better ability to distinguish between positive and negative instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0232ce4-3bc9-4151-b2d4-7e880825e0f9",
   "metadata": {},
   "source": [
    "### Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\n",
    "\n",
    "### Ans:-\n",
    "An example of a confusion matrix and demonstrate how precision, recall, and the F1 score can be calculated from it.\n",
    "\n",
    "Assume we have a binary classification problem where we are trying to classify whether an email is spam or not. We've built a classification model and tested it on a dataset with 100 email samples. Here's the confusion matrix:\n",
    "\n",
    "           Predicted Spam (Positive)   Predicted Not Spam (Negative)\n",
    "Actual Spam (Positive)  42                     8\n",
    "\n",
    "Actual Not Spam (Negative) 12                    38\n",
    "\n",
    "\n",
    "In this confusion matrix:\n",
    "- True Positives (TP): 42 emails were correctly classified as spam.\n",
    "- True Negatives (TN): 38 emails were correctly classified as not spam.\n",
    "- False Positives (FP): 8 emails were incorrectly classified as spam (they were not spam).\n",
    "- False Negatives (FN): 12 emails were incorrectly classified as not spam (they were spam).\n",
    "\n",
    "**Now, let's calculate precision, recall, and the F1 score:**\n",
    "\n",
    "1. Precision:\n",
    "Precision measures how many of the predicted positive instances were actually positive. It's calculated as:\n",
    "**Precision = TN / TP + Fp = 42/42+8 = 0.84**\n",
    "\n",
    "So, the precision is 0.84 or 84%. This means that when the model predicts an email as spam, it is correct 84% of the time.\n",
    "\n",
    "2. Recall:\n",
    "Recall measures how many of the actual positive instances were correctly predicted as positive. It's calculated as:\n",
    "**Recall = TP / TP+FN = 42/42+12 = 0.777**\n",
    "\n",
    "So, the recall is 0.777 or approximately 77.7%. This means the model correctly identifies about 77.7% of all the spam emails in the dataset.\n",
    "\n",
    "3. F1 Score:\n",
    "The F1 score is the harmonic mean of precision and recall and provides a balance between the two metrics. It's calculated as:\n",
    "**F1 = 2.Precision.Recall / Precision + Recall = 2.0.84.0.777 / 0.84+0.777 = 0.807**\n",
    "\n",
    "The F1 score is approximately 0.807 or 80.7%. It considers both precision and recall and is useful when you want to strike a balance between identifying as many positive instances as possible while minimizing false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f39338-7421-4b77-a508-1d964bbc6027",
   "metadata": {},
   "source": [
    "### Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n",
    "\n",
    "### Ans:-\n",
    "Choosing an appropriate evaluation metric for a classification problem is crucial because it helps you assess how well your model is performing and aligns your model evaluation with your specific goals and the characteristics of your problem. Different metrics focus on different aspects of classification performance, and the choice of metric should depend on the nature of your problem, the class distribution, and the consequences of different types of errors.\n",
    "\n",
    "**Importance of Choosing the Right Metric:**\n",
    "\n",
    "1. Aligns with Business Goals: Your choice of metric should align with the broader goals of your project. For example, in a medical diagnosis application, correctly identifying rare diseases might be more critical than minimizing false alarms.\n",
    "\n",
    "2. Handles Imbalanced Datasets: In imbalanced datasets where one class significantly outnumbers the other, accuracy can be misleading. Choosing metrics like precision, recall, F1-score, or area under the ROC curve (AUC-ROC) can provide a more balanced view of performance.\n",
    "\n",
    "3. Considers the Cost of Errors: Different classification errors might have different consequences. For instance, in fraud detection, a false negative (not detecting a fraud case) could be more costly than a false positive (flagging a non-fraudulent transaction).\n",
    "\n",
    "4. Reflects Data Characteristics: Some metrics are more suitable for specific data characteristics. For instance, the ROC-AUC is often used when assessing models on highly imbalanced datasets.\n",
    "\n",
    "**How to Choose the Right Metric:**\n",
    "\n",
    "1. Understand Your Problem:\n",
    "\n",
    "- Start by understanding the nature of your classification problem. What are the consequences of different types of errors? What are your priorities - minimizing false positives, false negatives, or achieving a balance?\n",
    "- Consider domain knowledge and the specific goals of your project.\n",
    "\n",
    "2. Analyze Class Distribution:\n",
    "\n",
    "- Examine the class distribution in your dataset. If one class dominates, accuracy may not be informative, and you should consider metrics that account for imbalance.\n",
    "\n",
    "3. Define Success Criteria:\n",
    "\n",
    "- Clearly define what success means for your model. Is it more important to catch all positive cases (high recall) or minimize false positives (high precision)?\n",
    "- Consider creating a confusion matrix and calculating multiple metrics to understand trade-offs.\n",
    "\n",
    "4. Use Case Examples:\n",
    "\n",
    "- Accuracy: Suitable for balanced datasets or when false positives and false negatives have similar consequences.\n",
    "\n",
    "- Precision: Use when minimizing false positives is critical (e.g., spam detection, medical diagnoses).\n",
    "\n",
    "- Recall: Use when capturing all positive instances is crucial (e.g., disease detection, fault detection).\n",
    "\n",
    "- F1 Score: A balance between precision and recall, useful when you want to consider both types of errors equally.\n",
    "\n",
    "- ROC-AUC: Useful for imbalanced datasets or when you want to evaluate how well your model ranks positive instances compared to negative ones.\n",
    "\n",
    "- Specificity: Relevant when you want to focus on the performance of the negative class.\n",
    "\n",
    "5. Experiment and Compare: It's often a good practice to experiment with different metrics and assess their impact on your model's performance. You might find that optimizing for one metric results in trade-offs with another.\n",
    "\n",
    "6. Cross-Validation: When assessing model performance, use techniques like cross-validation to get a more robust estimate of how well your model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba0cc8-7d3e-4295-8824-789af12dbab8",
   "metadata": {},
   "source": [
    "### Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.\n",
    "\n",
    "### Ans:-\n",
    "One example of a classification problem where precision is the most important metric is in the context of medical diagnoses, particularly for diseases with severe consequences and where false positives can lead to unnecessary treatments or interventions. Let's consider a specific scenario:\n",
    "\n",
    "**Medical Diagnosis - Cancer Screening**\n",
    "\n",
    "Imagine a machine learning model designed to assist in the early detection of a specific type of cancer, such as breast cancer. In this scenario, precision is of paramount importance due to the following reasons:\n",
    "\n",
    "1. Consequences of False Positives:\n",
    "\n",
    "- A false positive in cancer diagnosis means that the model incorrectly predicts that a patient has cancer when they do not. This could lead to unnecessary stress, further invasive tests, and even treatments like chemotherapy or surgery, which have physical and emotional consequences for the patient.\n",
    "\n",
    "2. Emphasis on Minimizing Harm:\n",
    "\n",
    "- Medical ethics and the principle of \"do no harm\" underscore the importance of minimizing false positives, as they directly affect patients' well-being.\n",
    "\n",
    "3. High Stakes:\n",
    "\n",
    "- Cancer diagnoses, especially for aggressive forms of cancer, can be life-altering. False positives can lead to patients undergoing unnecessary and potentially harmful treatments, causing physical discomfort, emotional distress, and financial burdens.\n",
    "\n",
    "4. Resource Allocation:\n",
    "\n",
    "- Healthcare resources, including medical staff, facilities, and funds, are limited. A high rate of false positives can strain these resources, diverting them from patients who genuinely need them.\n",
    "\n",
    "5. Patient Trust and Acceptance:\n",
    "\n",
    "- A high rate of false positives can erode patient trust in the healthcare system and AI-assisted diagnoses. Patients may become hesitant to follow up on recommendations or seek medical advice, potentially delaying the detection of real cases.\n",
    "\n",
    "In this context, precision is the relevant metric because it quantifies the percentage of positive predictions that are actually correct. Maximizing precision means that the model minimizes the chances of false positives, providing more confidence to patients and healthcare providers when a positive diagnosis is made. It ensures that the model's predictions are as accurate as possible and that patients who receive positive diagnoses genuinely require further attention and care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c9953-87e6-4987-bca1-237851f4d294",
   "metadata": {},
   "source": [
    "### Q9. Provide an example of a classification problem where recall is the most important metric, and explain why.\n",
    "\n",
    "### Ans:-\n",
    "An example of a classification problem where recall is the most important metric is in the context of spam email detection. In email filtering systems, the priority is often on capturing as many spam emails as possible while allowing legitimate emails (ham) to pass through. Here's why recall is crucial in this scenario:\n",
    "\n",
    "**Spam Email Detection**\n",
    "\n",
    "1. Consequences of False Negatives:\n",
    "- False negatives in spam email detection occur when a spam email is incorrectly classified as a legitimate email and allowed into the user's inbox. These false negatives can result in users receiving unsolicited and potentially harmful content, including phishing attempts, malware, or fraudulent messages.\n",
    "\n",
    "2. User Experience and Trust:\n",
    "- Users rely on spam filters to keep their email inboxes clean and safe. A high false negative rate can erode trust in the email service or filter, as users may perceive it as ineffective in blocking spam.\n",
    "\n",
    "3. Security and Privacy:\n",
    "- Many spam emails are designed to deceive or exploit users, potentially leading to security breaches or privacy violations. Preventing these threats is a top priority in email security.\n",
    "\n",
    "4. Costs of Manual Intervention:\n",
    "- When users receive spam in their inboxes, they often have to manually review and delete these emails. This can be time-consuming and frustrating. High recall reduces the need for such manual interventions.\n",
    "\n",
    "5. Regulatory Compliance:\n",
    "- In some industries, regulatory compliance requires organizations to have robust email security measures in place. Ensuring a high recall rate is essential for compliance with such regulations.\n",
    "\n",
    "In this context, recall (also known as sensitivity) is the relevant metric because it quantifies the percentage of actual spam emails that the model correctly identifies as spam. Maximizing recall ensures that as many spam emails as possible are captured, minimizing the chances of false negatives.\n",
    "\n",
    "However, achieving high recall often comes at the cost of precision. Precision measures the accuracy of positive predictions (emails classified as spam), and maximizing it may result in some legitimate emails being incorrectly classified as spam (false positives). While precision is also important, in spam email detection, the focus is primarily on reducing false negatives to enhance email security and user experience.\n",
    "\n",
    "Balancing precision and recall is an ongoing challenge in spam email detection. Adjusting the model's threshold for classifying emails as spam can help strike the right balance between capturing more spam (higher recall) and avoiding false positives (higher precision), based on the specific needs and preferences of email service providers and users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
