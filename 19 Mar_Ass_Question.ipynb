{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e70340f-720f-4715-96a0-5e18f7bc509d",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "### Ans:-\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to bring all the features to a common scale, ensuring that no single feature dominates the learning process when using machine learning algorithms that rely on distance or magnitude, such as k-nearest neighbors or gradient descent-based algorithms.\n",
    "\n",
    ">Example:-\n",
    "\n",
    "Let's consider a dataset with a single numerical feature, \"Age,\" representing the age of individuals. The original \"Age\" values range from 20 to 60. We want to apply Min-Max scaling to bring these values into a range between 0 and 1.\n",
    "\n",
    "Original dataset (Age values):\n",
    "[20,30,40,50,60]\n",
    "\n",
    "To apply Min-Max scaling to this dataset, follow these steps:\n",
    "1. To apply Min-Max scaling to this dataset, follow these steps:\n",
    "   Xmin = 20\n",
    "   Xmax = 60\n",
    "2. Apply the Min-Max scaling formula to each data point in the dataset:\n",
    "                  \n",
    "   Xnormalized = X-Xmin /  Xmax - Xmin\n",
    "                 \n",
    "                  \n",
    "   For each value in the dataset:\n",
    "                             \n",
    "    for X=20, Xnormalized = 20-20/60-20 = 0.0\n",
    "                             \n",
    "    for X=20, Xnormalized = 30-20/60-20 = 0.1\n",
    "    \n",
    "    for x=40, Xnormalized = 40-20/60-20 = 0.2\n",
    "    \n",
    "    for x=50, Xnormalized = 50-20/60-20 = 0.3\n",
    "    \n",
    "    for x=60, Xnormalized = 60-20/60-20 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2357937-d754-4655-92f5-9044462b91fe",
   "metadata": {},
   "source": [
    "### Q2.What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "### Ans:-\n",
    "The Unit Vector technique, also known as normalization, is a feature scaling method used to scale the magnitude of numerical features to a unit norm. In this technique, each data point (vector) is scaled by dividing it by its Euclidean norm (L2 norm). The result is a vector of the same direction but with a magnitude of 1.\n",
    "\n",
    "The formula to normalize a single feature is as follows:\n",
    "  Xnormalized = X / |X|2\n",
    "  where X is the original value of the feature, and ∥X∥2 represents the         Euclidean norm of the feature vector\n",
    "  \n",
    ">The Unit Vector technique differs from Min-Max scaling in that Min-Max scaling transforms features to a specific range (e.g., 0 to 1), while the Unit Vector technique scales the magnitude of features to a unit norm. As a result, after applying Unit Vector scaling, each data point (feature vector) will have a magnitude of 1, but the direction of the data points will be preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7cf943-011a-4d6e-88de-9cf7aaeeb929",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "### Ans:-\n",
    "PCA, which stands for Principal Component Analysis, is a popular dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important patterns and variations in the data. It does this by identifying the principal components, which are orthogonal (uncorrelated) vectors that represent the directions of maximum variance in the data. By projecting the data onto these principal components, PCA reduces the number of features while retaining as much of the original information as possible.\n",
    "\n",
    "The main steps of PCA are as follows:-\n",
    "1. Compute the mean of each feature in the dataset.\n",
    "2. Center the data by subtracting the mean from each feature to make the data zero-centered.\n",
    "3. Calculate the covariance matrix of the centered data.\n",
    "4. Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "5. Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "6. Select the top k eigenvectors (principal components) that explain most of the variance. These k components form the reduced feature space.\n",
    "7. Project the data onto the reduced feature space to obtain the lower-dimensional representation.\n",
    "\n",
    "To apply PCA for dimensionality reduction, follow these steps:-\n",
    "\n",
    "Step 1: Center the data by subtracting the mean of each feature from the original dataset.\n",
    "\n",
    "Step 2: Calculate the covariance matrix of the centered data.\n",
    "\n",
    "Step 3: Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Step 4: Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\n",
    "Step 5: Select the top k eigenvectors (principal components) based on the explained variance. For this example, let's say we want to reduce the data to two dimensions (k=2).\n",
    "\n",
    "Step 6: Project the data onto the reduced feature space defined by the top two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d8574-e130-46ee-a49d-3da433944309",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "### Ans:-\n",
    "PCA and Feature Extraction are closely related concepts. In fact, PCA is a popular technique used for Feature Extraction, which is the process of transforming the original set of features into a new set of features that captures the most relevant information in the data.\n",
    "\n",
    "The main idea behind using PCA for Feature Extraction is to find a lower-dimensional representation of the data by identifying the principal components, which are orthogonal vectors representing the directions of maximum variance in the data. These principal components serve as new features that effectively summarize the important patterns and variations present in the original data.\n",
    "\n",
    "\n",
    "The steps involved in using PCA for Feature Extraction are the same as those mentioned earlier:-\n",
    "\n",
    "1. Center the data by subtracting the mean of each feature from the original dataset.\n",
    "2. Calculate the covariance matrix of the centered data.\n",
    "3. Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "4. Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "5. Select the top k eigenvectors (principal components) based on the explained variance, where k is the desired number of reduced dimensions (new features).\n",
    "6. Project the data onto the reduced feature space defined by the top k principal components.\n",
    "\n",
    "\n",
    "To use PCA for Feature Extraction, follow these steps:-\n",
    "\n",
    "1. Center the data by subtracting the mean of each feature from the original dataset.\n",
    "\n",
    "2. Calculate the covariance matrix of the centered data.\n",
    "\n",
    "3. Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4. Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\n",
    "5. Select the top k eigenvectors (principal components) based on the explained variance. For this example, let's say we want to reduce the data to two dimensions (k=2).\n",
    "\n",
    "6. Project the data onto the reduced feature space defined by the top two principal components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
